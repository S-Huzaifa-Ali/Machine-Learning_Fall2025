{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edd23c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1025 entries, 0 to 1024\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1025 non-null   int64  \n",
      " 1   sex       1025 non-null   int64  \n",
      " 2   cp        1025 non-null   int64  \n",
      " 3   trestbps  1025 non-null   int64  \n",
      " 4   chol      1025 non-null   int64  \n",
      " 5   fbs       1025 non-null   int64  \n",
      " 6   restecg   1025 non-null   int64  \n",
      " 7   thalach   1025 non-null   int64  \n",
      " 8   exang     1025 non-null   int64  \n",
      " 9   oldpeak   1025 non-null   float64\n",
      " 10  slope     1025 non-null   int64  \n",
      " 11  ca        1025 non-null   int64  \n",
      " 12  thal      1025 non-null   int64  \n",
      " 13  target    1025 non-null   int64  \n",
      "dtypes: float64(1), int64(13)\n",
      "memory usage: 112.2 KB\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "All Training and Testing Accuracies:\n",
      "       Penalty  Training Accuracy  Testing Accuracy\n",
      "0           L1           0.871951          0.795122\n",
      "1           L2           0.871951          0.795122\n",
      "2  Elastic Net           0.871951          0.795122\n"
     ]
    }
   ],
   "source": [
    "# Task 01\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv('heart.csv')\n",
    "data.info()\n",
    "data.describe()\n",
    "print(data.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "# L1 regularization\n",
    "\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "\n",
    "model_l1.fit(X_train, y_train)\n",
    "train_acc_l1 = accuracy_score(y_train, model_l1.predict(X_train))\n",
    "test_acc_l1 = accuracy_score(y_test, model_l1.predict(X_test))\n",
    "\n",
    "results.append({'Penalty': 'L1', 'Training Accuracy': train_acc_l1, 'Testing Accuracy': test_acc_l1})\n",
    "\n",
    "\n",
    "# L2 regularization\n",
    "\n",
    "model_l2 = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "model_l2.fit(X_train, y_train)\n",
    "train_acc_l2 = accuracy_score(y_train, model_l2.predict(X_train))\n",
    "test_acc_l2 = accuracy_score(y_test, model_l2.predict(X_test))\n",
    "\n",
    "results.append({'Penalty': 'L2', 'Training Accuracy': train_acc_l2, 'Testing Accuracy': test_acc_l2})\n",
    "\n",
    "\n",
    "# Elastic Net regularization\n",
    "\n",
    "model_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5)\n",
    "\n",
    "model_elasticnet.fit(X_train, y_train)\n",
    "train_acc_elasticnet = accuracy_score(y_train, model_elasticnet.predict(X_train))\n",
    "test_acc_elasticnet = accuracy_score(y_test, model_elasticnet.predict(X_test))\n",
    "results.append({'Penalty': 'Elastic Net', 'Training Accuracy': train_acc_elasticnet, 'Testing Accuracy': test_acc_elasticnet})\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nAll Training and Testing Accuracies:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a92024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Solver  Training Accuracy  Testing Accuracy\n",
      "0            lbfgs           0.975000               1.0\n",
      "1        liblinear           0.958333               1.0\n",
      "2        newton-cg           0.975000               1.0\n",
      "3  newton-cholesky           0.975000               1.0\n",
      "4              sag           0.983333               1.0\n",
      "5             saga           0.975000               1.0\n",
      "            Solver  Training Accuracy  Testing Accuracy\n",
      "0            lbfgs           0.871951          0.795122\n",
      "1        liblinear           0.871951          0.795122\n",
      "2        newton-cg           0.871951          0.795122\n",
      "3  newton-cholesky           0.871951          0.795122\n",
      "4              sag           0.871951          0.795122\n",
      "5             saga           0.871951          0.795122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Task 02\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "solvers = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "results = []\n",
    "\n",
    "for s in solvers:\n",
    "    model = LogisticRegression(solver=s)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    results.append({'Solver': s, 'Training Accuracy': train_accuracy, 'Testing Accuracy': test_accuracy})\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "# Testing on heart dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('heart.csv')\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "results_heart = []\n",
    "\n",
    "for s in solvers:\n",
    "    \n",
    "    model = LogisticRegression(solver=s, penalty='l2', max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    train_accuracy = accuracy_score(y_train, model.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    results_heart.append({'Solver': s, 'Training Accuracy': train_accuracy, 'Testing Accuracy': test_accuracy})\n",
    "\n",
    "\n",
    "results_heart_df = pd.DataFrame(results_heart)\n",
    "print(results_heart_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a79975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Result:\n",
      "Training Accuracy: 0.975\n",
      "Testing Accuracy: 1.0\n",
      "\n",
      "Perceptron Result:\n",
      "Training Accuracy: 0.675\n",
      "Testing Accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Task 03\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Model\n",
    "lr_model = LogisticRegression(solver='lbfgs')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "lr_train_acc = accuracy_score(y_train, lr_model.predict(X_train))\n",
    "lr_test_acc = accuracy_score(y_test, lr_model.predict(X_test))\n",
    "print(\"Logistic Regression Result:\")\n",
    "print(f\"Training Accuracy: {lr_train_acc:}\")\n",
    "print(f\"Testing Accuracy: {lr_test_acc:}\")\n",
    "\n",
    "# Perceptron Model\n",
    "perc_model = Perceptron(max_iter=1000, random_state=42)\n",
    "perc_model.fit(X_train, y_train)\n",
    "\n",
    "perc_train_acc = accuracy_score(y_train, perc_model.predict(X_train))\n",
    "perc_test_acc = accuracy_score(y_test, perc_model.predict(X_test))\n",
    "print(\"\\nPerceptron Result:\")\n",
    "print(f\"Training Accuracy: {perc_train_acc:}\")\n",
    "print(f\"Testing Accuracy: {perc_test_acc:}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf51c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Profession          10000 non-null  object\n",
      " 1   Income              10000 non-null  int64 \n",
      " 2   Credit_card_number  10000 non-null  int64 \n",
      " 3   Expiry              10000 non-null  object\n",
      " 4   Security_code       10000 non-null  int64 \n",
      " 5   Fraud               10000 non-null  int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 468.9+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.4981 - loss: 0.6960 - val_accuracy: 0.5119 - val_loss: 0.6940\n",
      "Epoch 2/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5113 - loss: 0.6944 - val_accuracy: 0.4944 - val_loss: 0.6942\n",
      "Epoch 3/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5194 - loss: 0.6928 - val_accuracy: 0.5125 - val_loss: 0.6930\n",
      "Epoch 4/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5133 - loss: 0.6933 - val_accuracy: 0.5150 - val_loss: 0.6935\n",
      "Epoch 5/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5153 - loss: 0.6924 - val_accuracy: 0.5013 - val_loss: 0.6931\n",
      "Epoch 6/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5194 - loss: 0.6922 - val_accuracy: 0.4950 - val_loss: 0.6947\n",
      "Epoch 7/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5206 - loss: 0.6924 - val_accuracy: 0.5144 - val_loss: 0.6931\n",
      "Epoch 8/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5206 - loss: 0.6918 - val_accuracy: 0.4956 - val_loss: 0.6944\n",
      "Epoch 9/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5147 - loss: 0.6920 - val_accuracy: 0.5088 - val_loss: 0.6937\n",
      "Epoch 10/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5217 - loss: 0.6921 - val_accuracy: 0.4981 - val_loss: 0.6929\n",
      "Epoch 11/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5252 - loss: 0.6917 - val_accuracy: 0.5131 - val_loss: 0.6936\n",
      "Epoch 12/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5228 - loss: 0.6918 - val_accuracy: 0.5038 - val_loss: 0.6939\n",
      "Epoch 13/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5206 - loss: 0.6916 - val_accuracy: 0.5031 - val_loss: 0.6943\n",
      "Epoch 14/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5259 - loss: 0.6915 - val_accuracy: 0.5081 - val_loss: 0.6943\n",
      "Epoch 15/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5208 - loss: 0.6919 - val_accuracy: 0.4988 - val_loss: 0.6935\n",
      "Epoch 16/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5220 - loss: 0.6916 - val_accuracy: 0.4994 - val_loss: 0.6939\n",
      "Epoch 17/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5256 - loss: 0.6911 - val_accuracy: 0.4888 - val_loss: 0.6933\n",
      "Epoch 18/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5252 - loss: 0.6911 - val_accuracy: 0.4969 - val_loss: 0.6948\n",
      "Epoch 19/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5306 - loss: 0.6913 - val_accuracy: 0.5013 - val_loss: 0.6935\n",
      "Epoch 20/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5242 - loss: 0.6914 - val_accuracy: 0.5131 - val_loss: 0.6942\n",
      "Epoch 21/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5252 - loss: 0.6911 - val_accuracy: 0.5063 - val_loss: 0.6953\n",
      "Epoch 22/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5294 - loss: 0.6909 - val_accuracy: 0.5050 - val_loss: 0.6947\n",
      "Epoch 23/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5277 - loss: 0.6909 - val_accuracy: 0.5006 - val_loss: 0.6944\n",
      "Epoch 24/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5298 - loss: 0.6910 - val_accuracy: 0.4963 - val_loss: 0.6958\n",
      "Epoch 25/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5227 - loss: 0.6911 - val_accuracy: 0.5025 - val_loss: 0.6948\n",
      "Epoch 26/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5263 - loss: 0.6904 - val_accuracy: 0.5038 - val_loss: 0.6932\n",
      "Epoch 27/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5292 - loss: 0.6909 - val_accuracy: 0.5113 - val_loss: 0.6970\n",
      "Epoch 28/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5273 - loss: 0.6909 - val_accuracy: 0.4931 - val_loss: 0.6942\n",
      "Epoch 29/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5305 - loss: 0.6904 - val_accuracy: 0.5006 - val_loss: 0.6964\n",
      "Epoch 30/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5241 - loss: 0.6907 - val_accuracy: 0.4931 - val_loss: 0.6950\n",
      "Epoch 31/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5291 - loss: 0.6906 - val_accuracy: 0.4944 - val_loss: 0.6960\n",
      "Epoch 32/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5269 - loss: 0.6906 - val_accuracy: 0.4994 - val_loss: 0.6957\n",
      "Epoch 33/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5267 - loss: 0.6901 - val_accuracy: 0.5069 - val_loss: 0.6939\n",
      "Epoch 34/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5238 - loss: 0.6907 - val_accuracy: 0.4875 - val_loss: 0.6953\n",
      "Epoch 35/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5192 - loss: 0.6905 - val_accuracy: 0.5044 - val_loss: 0.6966\n",
      "Epoch 36/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5272 - loss: 0.6906 - val_accuracy: 0.5044 - val_loss: 0.6956\n",
      "Epoch 37/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5320 - loss: 0.6902 - val_accuracy: 0.5094 - val_loss: 0.6944\n",
      "Epoch 38/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5392 - loss: 0.6904 - val_accuracy: 0.5031 - val_loss: 0.6946\n",
      "Epoch 39/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5270 - loss: 0.6900 - val_accuracy: 0.4963 - val_loss: 0.6950\n",
      "Epoch 40/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5308 - loss: 0.6903 - val_accuracy: 0.4938 - val_loss: 0.6952\n",
      "Epoch 41/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5320 - loss: 0.6897 - val_accuracy: 0.5131 - val_loss: 0.6949\n",
      "Epoch 42/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5306 - loss: 0.6903 - val_accuracy: 0.5056 - val_loss: 0.6967\n",
      "Epoch 43/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5353 - loss: 0.6898 - val_accuracy: 0.5069 - val_loss: 0.6950\n",
      "Epoch 44/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5286 - loss: 0.6902 - val_accuracy: 0.5031 - val_loss: 0.6956\n",
      "Epoch 45/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5381 - loss: 0.6900 - val_accuracy: 0.5006 - val_loss: 0.6968\n",
      "Epoch 46/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5323 - loss: 0.6903 - val_accuracy: 0.5038 - val_loss: 0.6955\n",
      "Epoch 47/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5345 - loss: 0.6896 - val_accuracy: 0.5019 - val_loss: 0.6962\n",
      "Epoch 48/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.5302 - loss: 0.6899 - val_accuracy: 0.5056 - val_loss: 0.6950\n",
      "Epoch 49/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5366 - loss: 0.6899 - val_accuracy: 0.5000 - val_loss: 0.6963\n",
      "Epoch 50/50\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5317 - loss: 0.6900 - val_accuracy: 0.5013 - val_loss: 0.6971\n",
      "\n",
      "Model Evaluation on Test Data:\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4935 - loss: 0.6989\n",
      "Test Loss: 0.6989365816116333\n",
      "Test Accuracy: 0.4934999942779541\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "Accuracy: 0.4935\n",
      "Precision: 0.49547920433996384\n",
      "Recall: 0.5463609172482552\n",
      "F1 Score: 0.5196775723091512\n"
     ]
    }
   ],
   "source": [
    "# Task 04\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "data = pd.read_csv('fraud_detection.csv')\n",
    "\n",
    "data.info()\n",
    "data.head()\n",
    "data.describe()\n",
    "\n",
    "\n",
    "data = data.drop(['Credit_card_number', 'Expiry'], axis=1)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['Profession'] = label_encoder.fit_transform(data['Profession'])\n",
    "\n",
    "X = data.drop('Fraud', axis=1)\n",
    "y = data['Fraud']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1])) # Hidden Layer 1\n",
    "model.add(Dense(64, activation='tanh')) # Hidden Layer 2\n",
    "model.add(Dense(32, activation='relu')) # Hidden Layer 3\n",
    "model.add(Dense(1, activation='sigmoid')) # Output Layer (Binary)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\nModel Evaluation on Test Data:\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26510d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1143 entries, 0 to 1142\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1143 non-null   float64\n",
      " 1   volatile acidity      1143 non-null   float64\n",
      " 2   citric acid           1143 non-null   float64\n",
      " 3   residual sugar        1143 non-null   float64\n",
      " 4   chlorides             1143 non-null   float64\n",
      " 5   free sulfur dioxide   1143 non-null   float64\n",
      " 6   total sulfur dioxide  1143 non-null   float64\n",
      " 7   density               1143 non-null   float64\n",
      " 8   pH                    1143 non-null   float64\n",
      " 9   sulphates             1143 non-null   float64\n",
      " 10  alcohol               1143 non-null   float64\n",
      " 11  quality               1143 non-null   int64  \n",
      " 12  Id                    1143 non-null   int64  \n",
      "dtypes: float64(11), int64(2)\n",
      "memory usage: 116.2 KB\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.4036 - loss: 1.7313 - val_accuracy: 0.5355 - val_loss: 1.3197\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5581 - loss: 1.1741 - val_accuracy: 0.5027 - val_loss: 1.1235\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5964 - loss: 1.0414 - val_accuracy: 0.5956 - val_loss: 1.0208\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6142 - loss: 0.9924 - val_accuracy: 0.5246 - val_loss: 1.0359\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6115 - loss: 0.9651 - val_accuracy: 0.5738 - val_loss: 0.9953\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6361 - loss: 0.9423 - val_accuracy: 0.5683 - val_loss: 0.9829\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6320 - loss: 0.9205 - val_accuracy: 0.5683 - val_loss: 0.9910\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6443 - loss: 0.9099 - val_accuracy: 0.5628 - val_loss: 1.0041\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6498 - loss: 0.8920 - val_accuracy: 0.6066 - val_loss: 0.9665\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6662 - loss: 0.8656 - val_accuracy: 0.6066 - val_loss: 0.9733\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6539 - loss: 0.8570 - val_accuracy: 0.5847 - val_loss: 0.9719\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6648 - loss: 0.8443 - val_accuracy: 0.6066 - val_loss: 0.9675\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6676 - loss: 0.8271 - val_accuracy: 0.6066 - val_loss: 0.9918\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6635 - loss: 0.8275 - val_accuracy: 0.6011 - val_loss: 0.9940\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6799 - loss: 0.8086 - val_accuracy: 0.5902 - val_loss: 1.0051\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6758 - loss: 0.8016 - val_accuracy: 0.6011 - val_loss: 1.0031\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6785 - loss: 0.7886 - val_accuracy: 0.6066 - val_loss: 1.0073\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6867 - loss: 0.7728 - val_accuracy: 0.5956 - val_loss: 0.9935\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6895 - loss: 0.7804 - val_accuracy: 0.6011 - val_loss: 0.9910\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6963 - loss: 0.7639 - val_accuracy: 0.6175 - val_loss: 1.0070\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7127 - loss: 0.7446 - val_accuracy: 0.6066 - val_loss: 1.0321\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7004 - loss: 0.7363 - val_accuracy: 0.5847 - val_loss: 1.0260\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6977 - loss: 0.7304 - val_accuracy: 0.5956 - val_loss: 1.0360\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7045 - loss: 0.7159 - val_accuracy: 0.5956 - val_loss: 1.0242\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7168 - loss: 0.7063 - val_accuracy: 0.6011 - val_loss: 1.0706\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7182 - loss: 0.6933 - val_accuracy: 0.5956 - val_loss: 1.0533\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7250 - loss: 0.6834 - val_accuracy: 0.6230 - val_loss: 1.0715\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7250 - loss: 0.6805 - val_accuracy: 0.6011 - val_loss: 1.0565\n",
      "Epoch 29/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7196 - loss: 0.6664 - val_accuracy: 0.5902 - val_loss: 1.0909\n",
      "Epoch 30/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7291 - loss: 0.6624 - val_accuracy: 0.6011 - val_loss: 1.0849\n",
      "Epoch 31/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7332 - loss: 0.6534 - val_accuracy: 0.5574 - val_loss: 1.1248\n",
      "Epoch 32/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7168 - loss: 0.6518 - val_accuracy: 0.6011 - val_loss: 1.1184\n",
      "Epoch 33/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7497 - loss: 0.6387 - val_accuracy: 0.5792 - val_loss: 1.1614\n",
      "Epoch 34/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7373 - loss: 0.6305 - val_accuracy: 0.5956 - val_loss: 1.1263\n",
      "Epoch 35/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7565 - loss: 0.6088 - val_accuracy: 0.5902 - val_loss: 1.1410\n",
      "Epoch 36/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7510 - loss: 0.6104 - val_accuracy: 0.5792 - val_loss: 1.2067\n",
      "Epoch 37/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7825 - loss: 0.5888 - val_accuracy: 0.6175 - val_loss: 1.1587\n",
      "Epoch 38/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7688 - loss: 0.5842 - val_accuracy: 0.6175 - val_loss: 1.1464\n",
      "Epoch 39/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7633 - loss: 0.5795 - val_accuracy: 0.5956 - val_loss: 1.1754\n",
      "Epoch 40/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7647 - loss: 0.5698 - val_accuracy: 0.5847 - val_loss: 1.1958\n",
      "Epoch 41/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7770 - loss: 0.5509 - val_accuracy: 0.5902 - val_loss: 1.2320\n",
      "Epoch 42/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7893 - loss: 0.5492 - val_accuracy: 0.5738 - val_loss: 1.2360\n",
      "Epoch 43/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7962 - loss: 0.5358 - val_accuracy: 0.5902 - val_loss: 1.2428\n",
      "Epoch 44/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7866 - loss: 0.5332 - val_accuracy: 0.5464 - val_loss: 1.3103\n",
      "Epoch 45/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8140 - loss: 0.5206 - val_accuracy: 0.5902 - val_loss: 1.2825\n",
      "Epoch 46/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8140 - loss: 0.5063 - val_accuracy: 0.5792 - val_loss: 1.2525\n",
      "Epoch 47/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8016 - loss: 0.5210 - val_accuracy: 0.5792 - val_loss: 1.2877\n",
      "Epoch 48/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8126 - loss: 0.4945 - val_accuracy: 0.5683 - val_loss: 1.3036\n",
      "Epoch 49/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8181 - loss: 0.4897 - val_accuracy: 0.5956 - val_loss: 1.3200\n",
      "Epoch 50/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8249 - loss: 0.4759 - val_accuracy: 0.5519 - val_loss: 1.3541\n",
      "\n",
      "Model Evaluation on Test Data:\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6201 - loss: 1.2226 \n",
      "Test Loss: 1.2225549221038818\n",
      "Test Accuracy: 0.6200873255729675\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Accuracy: 0.6200873362445415\n",
      "Precision: 0.6024891578251819\n",
      "Recall: 0.6200873362445415\n",
      "F1 Score: 0.6058976473240885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Task 05\n",
    "# Dataset 1\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "wine_data = pd.read_csv(\"WineQT.csv\")\n",
    "\n",
    "wine_data.info()\n",
    "wine_data.head()\n",
    "wine_data.describe()\n",
    "\n",
    "wine_data = wine_data.drop(columns=[\"Id\"]) \n",
    "wine_features = wine_data.drop(columns=[\"quality\"])\n",
    "wine_target = wine_data[\"quality\"]\n",
    "\n",
    "# One hot encoding for multi class classification\n",
    "wine_target = to_categorical(wine_target)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "wine_features = scaler.fit_transform(wine_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine_features, wine_target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))  # Hidden Layer 1\n",
    "model.add(Dense(64, activation='tanh'))  # Hidden Layer 2\n",
    "model.add(Dense(32, activation='relu'))  # Hidden Layer 3\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))  # Output Layer (Multi-class)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "print(\"\\nModel Evaluation on Test Data:\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "y_test_classes = y_test.argmax(axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test_classes, y_pred)\n",
    "precision = precision_score(y_test_classes, y_pred, average='weighted')\n",
    "recall = recall_score(y_test_classes, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test_classes, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e32253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4600 entries, 0 to 4599\n",
      "Data columns (total 18 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   date           4600 non-null   object \n",
      " 1   price          4600 non-null   float64\n",
      " 2   bedrooms       4600 non-null   float64\n",
      " 3   bathrooms      4600 non-null   float64\n",
      " 4   sqft_living    4600 non-null   int64  \n",
      " 5   sqft_lot       4600 non-null   int64  \n",
      " 6   floors         4600 non-null   float64\n",
      " 7   waterfront     4600 non-null   int64  \n",
      " 8   view           4600 non-null   int64  \n",
      " 9   condition      4600 non-null   int64  \n",
      " 10  sqft_above     4600 non-null   int64  \n",
      " 11  sqft_basement  4600 non-null   int64  \n",
      " 12  yr_built       4600 non-null   int64  \n",
      " 13  yr_renovated   4600 non-null   int64  \n",
      " 14  street         4600 non-null   object \n",
      " 15  city           4600 non-null   object \n",
      " 16  statezip       4600 non-null   object \n",
      " 17  country        4600 non-null   object \n",
      "dtypes: float64(4), int64(9), object(5)\n",
      "memory usage: 647.0+ KB\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shuza\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 448288882688.0000 - mean_squared_error: 448288882688.0000 - val_loss: 401475076096.0000 - val_mean_squared_error: 401475076096.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 448226361344.0000 - mean_squared_error: 448226361344.0000 - val_loss: 401394925568.0000 - val_mean_squared_error: 401394925568.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 448126779392.0000 - mean_squared_error: 448126779392.0000 - val_loss: 401278402560.0000 - val_mean_squared_error: 401278402560.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 447987580928.0000 - mean_squared_error: 447987580928.0000 - val_loss: 401121247232.0000 - val_mean_squared_error: 401121247232.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 447807651840.0000 - mean_squared_error: 447807651840.0000 - val_loss: 400925163520.0000 - val_mean_squared_error: 400925163520.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 447587549184.0000 - mean_squared_error: 447587549184.0000 - val_loss: 400688775168.0000 - val_mean_squared_error: 400688775168.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 447328190464.0000 - mean_squared_error: 447328190464.0000 - val_loss: 400414474240.0000 - val_mean_squared_error: 400414474240.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 447030558720.0000 - mean_squared_error: 447030558720.0000 - val_loss: 400103473152.0000 - val_mean_squared_error: 400103473152.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 446695899136.0000 - mean_squared_error: 446695899136.0000 - val_loss: 399757574144.0000 - val_mean_squared_error: 399757574144.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 446325456896.0000 - mean_squared_error: 446325456896.0000 - val_loss: 399376252928.0000 - val_mean_squared_error: 399376252928.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 445919952896.0000 - mean_squared_error: 445919952896.0000 - val_loss: 398962163712.0000 - val_mean_squared_error: 398962163712.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 445481189376.0000 - mean_squared_error: 445481189376.0000 - val_loss: 398514225152.0000 - val_mean_squared_error: 398514225152.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 445009035264.0000 - mean_squared_error: 445009035264.0000 - val_loss: 398035025920.0000 - val_mean_squared_error: 398035025920.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 444504801280.0000 - mean_squared_error: 444504801280.0000 - val_loss: 397526958080.0000 - val_mean_squared_error: 397526958080.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 443970420736.0000 - mean_squared_error: 443970420736.0000 - val_loss: 396985597952.0000 - val_mean_squared_error: 396985597952.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 443405664256.0000 - mean_squared_error: 443405664256.0000 - val_loss: 396416614400.0000 - val_mean_squared_error: 396416614400.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 442812268544.0000 - mean_squared_error: 442812268544.0000 - val_loss: 395819384832.0000 - val_mean_squared_error: 395819384832.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 442190364672.0000 - mean_squared_error: 442190364672.0000 - val_loss: 395196301312.0000 - val_mean_squared_error: 395196301312.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 441541001216.0000 - mean_squared_error: 441541001216.0000 - val_loss: 394545692672.0000 - val_mean_squared_error: 394545692672.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 440864800768.0000 - mean_squared_error: 440864800768.0000 - val_loss: 393868902400.0000 - val_mean_squared_error: 393868902400.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 440163270656.0000 - mean_squared_error: 440163270656.0000 - val_loss: 393169010688.0000 - val_mean_squared_error: 393169010688.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 439437950976.0000 - mean_squared_error: 439437950976.0000 - val_loss: 392443068416.0000 - val_mean_squared_error: 392443068416.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 438687629312.0000 - mean_squared_error: 438687629312.0000 - val_loss: 391697563648.0000 - val_mean_squared_error: 391697563648.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 437914337280.0000 - mean_squared_error: 437914337280.0000 - val_loss: 390923124736.0000 - val_mean_squared_error: 390923124736.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 437116403712.0000 - mean_squared_error: 437116403712.0000 - val_loss: 390132563968.0000 - val_mean_squared_error: 390132563968.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 436296843264.0000 - mean_squared_error: 436296843264.0000 - val_loss: 389315100672.0000 - val_mean_squared_error: 389315100672.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 435454476288.0000 - mean_squared_error: 435454476288.0000 - val_loss: 388481089536.0000 - val_mean_squared_error: 388481089536.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 434591629312.0000 - mean_squared_error: 434591629312.0000 - val_loss: 387619848192.0000 - val_mean_squared_error: 387619848192.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 433706532864.0000 - mean_squared_error: 433706532864.0000 - val_loss: 386745237504.0000 - val_mean_squared_error: 386745237504.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 432801808384.0000 - mean_squared_error: 432801808384.0000 - val_loss: 385849032704.0000 - val_mean_squared_error: 385849032704.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 431877226496.0000 - mean_squared_error: 431877226496.0000 - val_loss: 384926580736.0000 - val_mean_squared_error: 384926580736.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 430931804160.0000 - mean_squared_error: 430931804160.0000 - val_loss: 383989547008.0000 - val_mean_squared_error: 383989547008.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 429967507456.0000 - mean_squared_error: 429967507456.0000 - val_loss: 383033409536.0000 - val_mean_squared_error: 383033409536.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 428982239232.0000 - mean_squared_error: 428982239232.0000 - val_loss: 382065934336.0000 - val_mean_squared_error: 382065934336.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 427981668352.0000 - mean_squared_error: 427981668352.0000 - val_loss: 381064675328.0000 - val_mean_squared_error: 381064675328.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 426959601664.0000 - mean_squared_error: 426959601664.0000 - val_loss: 380059648000.0000 - val_mean_squared_error: 380059648000.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 425922035712.0000 - mean_squared_error: 425922035712.0000 - val_loss: 379028799488.0000 - val_mean_squared_error: 379028799488.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 424865726464.0000 - mean_squared_error: 424865726464.0000 - val_loss: 377986613248.0000 - val_mean_squared_error: 377986613248.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 423792476160.0000 - mean_squared_error: 423792476160.0000 - val_loss: 376930828288.0000 - val_mean_squared_error: 376930828288.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 422703923200.0000 - mean_squared_error: 422703923200.0000 - val_loss: 375846371328.0000 - val_mean_squared_error: 375846371328.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 421596528640.0000 - mean_squared_error: 421596528640.0000 - val_loss: 374760833024.0000 - val_mean_squared_error: 374760833024.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 420475338752.0000 - mean_squared_error: 420475338752.0000 - val_loss: 373646557184.0000 - val_mean_squared_error: 373646557184.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 419335471104.0000 - mean_squared_error: 419335471104.0000 - val_loss: 372525596672.0000 - val_mean_squared_error: 372525596672.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 418181087232.0000 - mean_squared_error: 418181087232.0000 - val_loss: 371387269120.0000 - val_mean_squared_error: 371387269120.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 417012154368.0000 - mean_squared_error: 417012154368.0000 - val_loss: 370226724864.0000 - val_mean_squared_error: 370226724864.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 415827263488.0000 - mean_squared_error: 415827263488.0000 - val_loss: 369057595392.0000 - val_mean_squared_error: 369057595392.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 414628151296.0000 - mean_squared_error: 414628151296.0000 - val_loss: 367872638976.0000 - val_mean_squared_error: 367872638976.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 413413343232.0000 - mean_squared_error: 413413343232.0000 - val_loss: 366680473600.0000 - val_mean_squared_error: 366680473600.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 412185526272.0000 - mean_squared_error: 412185526272.0000 - val_loss: 365467009024.0000 - val_mean_squared_error: 365467009024.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 410941489152.0000 - mean_squared_error: 410941489152.0000 - val_loss: 364244893696.0000 - val_mean_squared_error: 364244893696.0000\n",
      "\n",
      "Model Evaluation on Test Data:\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1316334206976.0000 - mean_squared_error: 1316334206976.0000\n",
      "Test Loss (MSE): 1316334206976.0\n",
      "WARNING:tensorflow:5 out of the last 72 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000015464E3CEA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Mean Squared Error (MSE): 1316334183337.955\n",
      "Mean Absolute Error (MAE): 545209.6553426124\n",
      "R2 Score: -0.2907181519381843\n"
     ]
    }
   ],
   "source": [
    "# Task 05\n",
    "# Dataset 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "data.info()\n",
    "data.head()\n",
    "data.describe()\n",
    "\n",
    "data = data.drop(columns=[\"date\", \"street\", \"city\", \"statezip\", \"country\"])\n",
    "\n",
    "data_features = data.drop(columns=[\"price\"])\n",
    "data_target = data[\"price\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_features = scaler.fit_transform(data_features)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_features, data_target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))  # Hidden Layer 1\n",
    "model.add(Dense(64, activation='tanh'))  # Hidden Layer 2\n",
    "model.add(Dense(32, activation='relu'))  # Hidden Layer 3\n",
    "model.add(Dense(1, activation='linear'))  # Output Layer for Regression\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "print(\"\\nModel Evaluation on Test Data:\")\n",
    "test_loss, test_mse = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R2 Score: {r2}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
